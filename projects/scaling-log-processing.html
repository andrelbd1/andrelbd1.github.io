<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>About André Luiz</title>
    <link rel="icon" type="image/x-icon" href="../assets/img/favicon.png" />
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v5.12.1/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="../css/styles.css" rel="stylesheet" />
</head>

<body id="page-top">
    <!-- Navigation-->
    <div id="navbar"></div>

    <section class="page-section bg-white portfolio-section" id="projects">
        <div class="container" style="margin-top: 27px;">
            <div class="text-center">
                <img class="img-fluid d-block mx-auto" src="../assets/img/projects/scaling-log-processing/logo.png" alt="" />
                <h2 class="section-heading text-uppercase">Scaling Log Processing</h2>
                <h3 class="section-subheading text-muted">Redesigning a high-volume ETL pipeline to reduce latency, cut costs, and enable faster analytics</h3>
            </div>
            <div class="text-left">
                <h4 class="section-heading text-capitalize">Overview</h4>
            </div>
            <div class="row justify-content-center">
                <div class="col-md-11">
                    <p class="text-muted-justified">
                        <b>(S)ituation:</b> At Seedz, a data-driven company operating across 10 countries and facilitating over BRL 10 billion in transactions, the existing data pipeline for processing loyalty program logs was inefficient. It relied on a scheduled AWS Fargate job to sequentially process logs from the previous day, leading to delays of up to 3.5 hours for 400,000 logs.
                    </p>
                    <p class="text-muted-justified">
                        <b>(T)ask:</b> As a Data Engineer in 2025, I was responsible for redesigning this pipeline to significantly reduce processing time, improve scalability, and cut infrastructure costs.
                    </p>
                    <p class="text-muted-justified">
                        <b>(A)ction:</b> I evaluated various solutions and designed a new batch-processing architecture using Python, multiprocessing, Docker, and Apache Airflow. Instead of multithreading, the pipeline adopted multiprocessing for greater parallelism and ran on EC2 instances orchestrated via ECS. Logs arriving in Amazon S3 triggered SQS messages, and the Airflow DAG, scheduled every 10 minutes, launched up to 11 containers to process logs in parallel. Transformed logs were stored back in S3 and ingested into Redshift for downstream analytics.
                    </p>
                    <p class="text-muted-justified">
                        <b>(R)esult:</b> The new solution reduced the processing time for 110,000 logs from 55 minutes to just 5 minutes—a 90% performance improvement. It also ensured cost-effective scalability, timely data availability, and better support for analytics dashboards and decision-making processes.
                    </p>
                </div>
            </div>
            <div class="text-left">
                <h4 class="section-heading text-capitalize">Full Report</h4>
            </div>
            <div class="row justify-content-center">
                <div class="col-md-11">
                    <p class="text-muted-justified">
                        At Seedz, we are committed to transforming the agribusiness ecosystem through data-driven solutions, such as a loyalty program that enhances sales campaigns by rewarding sellers and offering cashback to customers.
                        Operating in 10 countries, our platform has enabled financial transactions worth over BRL 10 billion.
                    </p>
                    <p class="text-muted-justified">
                        As a Data Engineer in 2025, I led the optimization of a data pipeline responsible for processing logs generated by our loyalty points engine and loading them into the data lake.
                        These logs, stored individually in an Amazon S3 bucket within date-partitioned folders in JSON format, required standardization and transformation into a more efficient format to facilitate downstream analytics.
                    </p>
                    <p class="text-muted-justified">
                        The legacy pipeline was inefficient.
                        It utilized an AWS Fargate job scheduled to run early the next day to sequentially process logs generated the previous day.
                        Despite employing 10 threads and leveraging Amazon Redshift for progress tracking, processing 110,000 logs still took up to 55 minutes.
                        With an average daily volume of 400,000 logs, the pipeline required up to 3.5 hours to complete, resulting in unacceptable latency for downstream analytics.
                    </p>
                    <p class="text-muted-justified">
                        To address the inefficiencies, I designed a cost-effective and high-performance batch-processing solution.
                        Since real-time streaming was not required, we focused on optimizing batch processing.
                        After evaluating AWS Glue and finding it cost-prohibitive, we opted for a Python-based pipeline leveraging multiprocessing for parallelism.
                        The solution was containerized with Docker, enabling horizontal scalability, and orchestrated using Apache Airflow.
                        The entire system was deployed on EC2 instances managed through Amazon ECS, ensuring flexibility and cost efficiency.
                    </p>
                    <p class="text-muted-justified">
                        Each log, in JSON format, triggers a message to an SQS queue upon arrival in S3.
                        As the frequecy of log generation was low, we scheduled Airflow DAG to check every 10 minutes for pending messages. 
                        If found, the DAG starts up to 11 containers, each processing 10,000 logs, allowing us to handle up to 110,000 logs every 10 minutes while staying within SQS’s 120,000 in-flight message limit.
                        The processed results are stored back in the data lake (S3) in a standardized and optimized format.
                        These results are then ingested into Amazon Redshift to support downstream analytics, including real-time dashboards and reporting, ensuring faster insights and improved decision-making.
                    </p>
                    <p class="text-muted-centered">
                        <img class="img-fluid d-block mx-auto" src="../assets/img/projects/scaling-log-processing/pipeline.png" alt="" />
                        <b>Figure 1:</b> Illustration of the data pipeline for processing logs from the loyalty program.
                    </p>
                    <p class="text-muted-justified">
                        Benchmark tests revealed significant performance improvements with the redesigned pipeline.
                        Using a container with 4 GB RAM and 2 vCPUs, single-threaded processing of 10,000 logs took 67 minutes.
                        By leveraging multiprocessing to split the workload into 10 parallel processes, the processing time was reduced to just 5 minutes, with each process consuming approximately 40 MB of memory.
                        Therefore, the entire pipeline could process 110,000 logs in 55 minutes, achieving a remarkable 90% reduction in processing time.
                        This optimization significantly improved the pipeline’s performance, enabling faster processing, greater scalability, reduced costs, and enhanced data availability for downstream analytics.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer-->
    <div id="footer"></div>

    <!-- Bootstrap core JS-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
    <!-- Third party plugin JS-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
    
    <script>
        const nav = ['navbar'];
        nav.forEach(b => {
            fetch(`${b}.html`)
                .then(response => response.text())
                .then(data => {
                    document.getElementById(`${b}`).outerHTML = data;
                })
                .catch(error => console.error(`Error loading ${b}:`, error));
        });

        const body = ['footer'];
        body.forEach(b => {
            fetch(`../body/${b}.html`)
                .then(response => response.text())
                .then(data => {
                    document.getElementById(`${b}`).outerHTML = data;
                })
                .catch(error => console.error(`Error loading ${b}:`, error));
        });
    </script>
</body>
</html>







